"""
çŸ¥è¯†åº“æ„å»ºå·¥å…· - ä¸»å…¥å£
ä»åŸå§‹æ•°æ®è‡ªåŠ¨æ„å»ºç»“æ„åŒ–çŸ¥è¯†åº“
"""
import argparse
import json
import os
from pathlib import Path
from typing import List, Dict

from parsers.document_parser import DocumentParser
from parsers.order_parser import OrderParser
from parsers.chat_parser import ChatParser
from extractors.product_extractor import ProductExtractor
from extractors.qa_extractor import QAExtractor
from analyzers.order_analyzer import OrderAnalyzer
from analyzers.knowledge_fusion import KnowledgeFusion


class KnowledgeBuilder:
    """çŸ¥è¯†åº“æ„å»ºå™¨"""
    
    def __init__(self, llm_api_key: str = None):
        self.doc_parser = DocumentParser()
        self.order_parser = OrderParser()
        self.chat_parser = ChatParser()
        self.product_extractor = ProductExtractor(llm_api_key)
        self.qa_extractor = QAExtractor(llm_api_key)
        self.order_analyzer = OrderAnalyzer()
        self.fusion = KnowledgeFusion()
        
    def build_from_documents(self, docs_dir: str) -> List[Dict]:
        """ä»äº§å“æ–‡æ¡£æ„å»ºçŸ¥è¯†"""
        products = []
        docs_path = Path(docs_dir)
        
        for file_path in docs_path.rglob('*'):
            if file_path.suffix.lower() in ['.pdf', '.docx', '.doc', '.txt', '.html']:
                print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {file_path}")
                text = self.doc_parser.parse(file_path)
                if text:
                    product = self.product_extractor.extract(text)
                    if product:
                        products.append(product)
                        
        return products
    
    def build_from_orders(self, orders_file: str) -> Dict:
        """ä»å†å²è®¢å•åˆ†æçŸ¥è¯†"""
        print(f"ğŸ“Š åˆ†æè®¢å•: {orders_file}")
        orders = self.order_parser.parse(orders_file)
        insights = self.order_analyzer.analyze(orders)
        return insights
    
    def build_from_chats(self, chats_file: str) -> List[Dict]:
        """ä»èŠå¤©è®°å½•æå–QA"""
        print(f"ğŸ’¬ å¤„ç†èŠå¤©è®°å½•: {chats_file}")
        conversations = self.chat_parser.parse(chats_file)
        qa_pairs = self.qa_extractor.extract(conversations)
        return qa_pairs
    
    def build(self, 
              products_dir: str = None,
              orders_file: str = None, 
              chats_file: str = None,
              output_file: str = "knowledge_base.json") -> Dict:
        """æ„å»ºå®Œæ•´çŸ¥è¯†åº“"""
        
        kb = {
            "products": [],
            "faqs": [],
            "policies": [],
            "metadata": {
                "sources": [],
                "generated_at": None,
                "stats": {}
            }
        }
        
        # 1. å¤„ç†äº§å“æ–‡æ¡£
        if products_dir and os.path.exists(products_dir):
            products = self.build_from_documents(products_dir)
            kb["products"] = products
            kb["metadata"]["sources"].append(f"products:{products_dir}")
            print(f"âœ… æå– {len(products)} ä¸ªäº§å“")
        
        # 2. åˆ†æè®¢å•æ•°æ®
        if orders_file and os.path.exists(orders_file):
            insights = self.build_from_orders(orders_file)
            # ä»æ´å¯Ÿç”ŸæˆFAQå’Œæ”¿ç­–
            kb["faqs"].extend(insights.get("faq_candidates", []))
            kb["policies"].extend(insights.get("policies", []))
            kb["metadata"]["sources"].append(f"orders:{orders_file}")
            print(f"âœ… ä»è®¢å•æå– {len(insights.get('faq_candidates', []))} ä¸ªFAQ")
        
        # 3. æå–å¯¹è¯QA
        if chats_file and os.path.exists(chats_file):
            qa_pairs = self.build_from_chats(chats_file)
            kb["faqs"].extend(qa_pairs)
            kb["metadata"]["sources"].append(f"chats:{chats_file}")
            print(f"âœ… ä»å¯¹è¯æå– {len(qa_pairs)} ä¸ªQAå¯¹")
        
        # 4. çŸ¥è¯†èåˆä¸å»é‡
        print("ğŸ”„ èåˆçŸ¥è¯†...")
        kb = self.fusion.fuse(kb)
        
        # 5. ç”Ÿæˆå…ƒæ•°æ®
        kb["metadata"]["generated_at"] = self._get_timestamp()
        kb["metadata"]["stats"] = {
            "products": len(kb["products"]),
            "faqs": len(kb["faqs"]),
            "policies": len(kb["policies"])
        }
        
        # 6. ä¿å­˜
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(kb, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ¨ çŸ¥è¯†åº“å·²ç”Ÿæˆ: {output_file}")
        print(f"   äº§å“: {kb['metadata']['stats']['products']}")
        print(f"   FAQ: {kb['metadata']['stats']['faqs']}")
        print(f"   æ”¿ç­–: {kb['metadata']['stats']['policies']}")
        
        return kb
    
    def _get_timestamp(self) -> str:
        from datetime import datetime
        return datetime.now().isoformat()


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨æ„å»ºæ—…æ¸¸å®¢æœçŸ¥è¯†åº“')
    parser.add_argument('--products-dir', '-p', help='äº§å“æ–‡æ¡£ç›®å½•')
    parser.add_argument('--orders-file', '-o', help='å†å²è®¢å•CSVæ–‡ä»¶')
    parser.add_argument('--chats-file', '-c', help='èŠå¤©è®°å½•JSONæ–‡ä»¶')
    parser.add_argument('--output', '-O', default='knowledge_base.json', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--llm-key', '-k', help='LLM API Key')
    
    args = parser.parse_args()
    
    if not any([args.products_dir, args.orders_file, args.chats_file]):
        parser.print_help()
        print("\nâŒ è‡³å°‘æŒ‡å®šä¸€ä¸ªæ•°æ®æº")
        return
    
    builder = KnowledgeBuilder(llm_api_key=args.llm_key)
    builder.build(
        products_dir=args.products_dir,
        orders_file=args.orders_file,
        chats_file=args.chats_file,
        output_file=args.output
    )


if __name__ == '__main__':
    main()
